\documentclass{llncs2e/llncs}

\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{bibentry}
\usepackage[numbers]{natbib}

% Perhaps shorten the title of one of the two parts
\title{Survey of a few data-parallel languages}
\subtitle{The need for nested data-parallelism} \author{Philip L. Carlsen \and Martin Dybdal \and Ken Friis Larsen}
\institute{University of Copenhagen \\ \email{plcplc@gmail.com,
    dybber@dybber.dk, kflarsen@diku.dk}}


\begin{document}
\maketitle

\begin{abstract}
  TODO
\end{abstract}

\section{Introduction}
In recent years \emph{graphics processing units} have become the most
cost-efficient choice of hardware when it comes to raw compute
power. They deliver much higher throughput than ordinary processors
through their massively data-parallel architecture.

Following this change in hardware, quite a number of new data-parallel
languages \cite{} have been announced which aims to make
\emph{graphics processing units} programming more
accessible. Currently, OpenCL and CUDA are still the most popular GPU
programming frameworks in both academia and industry. These two
languages are extension of C and C++ respectively and they both
require manual memory management \emph{TODO them bash some more}

In this paper we present our experience with applying a few functional
data-parallel languages to some real world problems. \emph{TODO go
  more in depth} Our main contributions are:
\begin{itemize}
\item 
\end{itemize}


The rest of the paper is structured as follows. \emph{TODO write outline}

% Disposition for introduction
% \begin{itemize}
% \item Motivate the interest in data-parallel languages
% \item Present the experiments we have performed in short
% \item List of contributions
% \item Overview of remaining paper
% \end{itemize}

\section{Case I: Sobol sequence generation}
\emph{Sobol sequences are .. low-discrepancy sequence ... suitable for
numerical integration and Monte Carlo methods ... eventually include
the plots ...}

We will only present a simple inductive formulation of the algorithm,
and it should be noted that more efficient implementations exists
\cite{bratley1988algorithm, hwy2011emerald}. A more detailed
discussion can be found in our Master's thesis
\cite{dybdalcarlsen2013thesis}.

The algorithm is seeded by a so called \emph{direction vector}. To
generate a Sobol sequence of $32$-bit numbers a direction vector of
length $32$ is required.  Lists of direction vectors are available
online \cite{homepage:sobol:directionvectors}. 

Let $v$ be a direction vector of length $32$, the corresponding
Sobol sequence is defined by:
$$x_i = v_1i_1 \oplus v_2i_2 \oplus \ldots \oplus v_{32}i_{32}$$
which translates to the corresponding formulation on Haskell lists:
\begin{verbatim}
  sobol :: [Int32] -> Int32 -> Float
  sobol v = normalise . foldl1 xor . zipWith (*) v . toBitVector
\end{verbatim}
To generate a complete sequence we can map:
\begin{verbatim}
  sobol1D :: Int32 -> [Int32] -> [Float]
  sobol1D m v = map (sobol v) [1..m]
\end{verbatim}
And higher-dimensional sequences can be generated by an additional map:
\begin{verbatim}
  sobolND :: Int32 -> [[Int32]] -> [[Float]]
  sobolND m vs = map (sobol1D m) vs
\end{verbatim}

We now turn to implement the same algorithm in Accelerate. It should
be noted that the problems we will present are not unique for
Accelerate. We would have more or less the same troubles doing it in
Nikola, Repa, ...  TODO. We use Accelerate because its types are
sufficiently simple. The type of Accelerate arrays are indexed by
their dimensionality, thus \verb|Array DIM2 Int32| is a two
dimensional array. Accelerate arrays can not be irregular.

In the first step, there are only minor changes in the type:
\begin{verbatim}
  sobolAcc :: Array DIM1 Int32 -> Exp Int32 -> Acc (Array DIM0 Float)
  sobolAcc v = normalise . foldl1 xor . zipWith (*) v . toBitVector
\end{verbatim}
The next step becomes more complicated though, as Accelerate forbids
nested maps. We will not be able to write \verb|map (sobolAccv)|. 
Instead we will have to push this map inside the definition of
\verb|sobolAcc|, a manual vectorisation.
\begin{verbatim}
  sobol1DAcc :: Exp Int32 -> Array DIM1 Int32 -> Acc (Array DIM1 Float)
  sobol1DAcc m v = normalise $ foldl1 xor $ zipWith (*) vRep bitVectors
    where
     Z :. i = arrayShape v
     vRep = replicate (constant $ Z :. m :. All) (use v)
     bitVectors = generate TODOTODOTODOTODO
\end{verbatim}
What we have done is to vectorise every operation used in
\verb|sobolAcc|, such that they operate on values of with an
additional dimension. 

If we wish to generate $n$-dimensional Sobol sequences, we will again
face the same barrier and we would have to do a vectorisation in the
direction vector argument of \verb|sobol1DAcc|. Where we had two
dimensional arrays we will have to use three dimensional arrays. We
will not present a \verb|sobolAccND|, but just mention that in all
parts of the sobol1DAcc above we would have to take the additional
dimension into account.
% \begin{verbatim}
%   sobolAccND :: Exp Int32 -> Array DIM1 Int32 -> Acc (Array DIM2 Float)
% \end{verbatim}

This example illustrates a problem in languages that disallow nested
array operations in small scale. The fact that we could not apply
\verb|sobolAcc| in the context that we wanted, but had to change its
inner workings showcases a lack of composability and abstraction in
such languages. In a larger setting, such as the one we will present
in the next section, the function that we want to map could be part of
a library which we could not modify ourselves, and thus we would have
to reimplement the complete functionality in our own program instead.

\emph{TODO what was the problem with using generate instead of map in the
above examples?}

\emph{TODO how does the CUDA code generated by Accelerate look after
  fusion in the above case? Nikola code is fused into a single kernel}


% Disposition for Case I: Sobol generation
% \begin{itemize}
% \item Introduce the simple inductive algorithm for computing
%   Sobol-sequences. Note that more efficient algorithms exists, but
%   we use this simple algorithm for illustrative purposes. See a more
%   detailed explanation in our master's thesis.
% \item Present how the algorithm would be written in a language
%   allowing nesting and compare with the Accelerate version
%   Write that the Nikola version is similar.
% \item We should compare the generated CUDA code by Nikola and
%   Accelerate and make a remark on that
% \item Introduce the concept of "manual vectorisation".
% \end{itemize}

\section{Case II: Binomial option pricing}
\begin{itemize}
\item Present binomial option pricing algorithm
\item Show how that would lead to host synchronization in each iteration
\item See if we can find some paper about the overhead incurred by
  extraneous kernel launches
\item Say that an alternative parallelization scheme prices several
  options simultaneously: portfolio pricing.
\item Show that we have a hard time expressing such a portfolio
  pricer, because of irregularity when performing the manual
  vectorisation
\item Present a benchmark showing the benefit of having portfolio
  pricer rather than pricing each option in their own kernels
  sequentially.
\end{itemize}

\section{Related work}
\begin{itemize}
\item Mention NESL, Data-parallel Haskell and GPU-NESL
\item Mention some of the languages we haven't tested. See if can put
  them in categories where the above problems are present and aren't,
  even though we haven't tested them as much.
\end{itemize}

\section{Future work}
\begin{itemize}
\item Extend the survey to cover additional languages and problems
\item Create a larger performance benchmark of the different languages
\item Look closer at GPU-NESL and whether it performs
\end{itemize}

\section{Conclusion}
Summarize.

\section{Acknowledgments}

\bibliographystyle{plainnat} 
\bibliography{../bibliography/bibliography}

\end{document}
