\documentclass{llncs2e/llncs}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[numbers]{natbib}

% Perhaps shorten the title of one of the two parts
\title{Survey of a few data-parallel languages}
\subtitle{The need for nested data-parallelism} \author{Philip L. Carlsen \and Martin Dybdal \and Ken Friis Larsen}
\institute{University of Copenhagen \\ \email{plcplc@gmail.com,
    dybber@dybber.dk, kflarsen@diku.dk}}

\begin{document}
\maketitle

\section{Introduction}
In recent years \emph{graphics processing units} have become a very
cost-efficient choice of hardware when it comes to raw compute
power. They deliver much higher throughput than ordinary processors
through their massively data-parallel architecture.

Following this change in hardware, quite a number of new data-parallel
languages \cite{} have been announced which aims to make
\emph{graphics processing units} programming more
accessible. Currently, OpenCL and CUDA are still the most popular GPU
programming frameworks in both academia and industry. These two
languages are extension of C and C++ respectively and they both
require manual memory management \emph{TODO them bash some more}

In this paper we present our experience with applying a few functional
data-parallel languages to some real world problems. \emph{TODO go
  more in depth} Our main contributions are:
\begin{itemize}
\item 
\end{itemize}

The rest of the paper is structured as follows. \emph{TODO write outline}

% Disposition for introduction
% \begin{itemize}
% \item Motivate the interest in data-parallel languages
% \item Present the experiments we have performed in short
% \item List of contributions
% \item Overview of remaining paper
% \end{itemize}

\section{Case I: Sobol sequence generation}
The first problem that we have used to evaluate the languages is a
random number generator. A \emph{Sobol sequence generator} is a so
called quasi-random number generator (QRNG), where the aim is to make
samples of low discrepancy, rather than provide good statistical
properties. QRNGs is thus in contrast with pseudo-random number
generators (PRNGs). A visual comparison of a Sobol-sequence and a
sequence generated by the popular Mersenne Twister PRNG is presented
in Figure \ref{fig:discrepancyplot}.
\begin{figure}
  \centering
  \begin{minipage}{0.45\linewidth}
    \begin{center}
      \includegraphics[width=\textwidth]{../report/graphics/2D-sobol-sequence.pdf}

      \hspace{0.55cm}\textbf{(a)}
    \end{center}
  \end{minipage}
  \begin{minipage}{0.45\linewidth}
    \begin{center}
      \includegraphics[width=\textwidth]{../report/graphics/2D-mersenne-sequence.pdf}

      \hspace{0.55cm}\textbf{(b)}
    \end{center}
  \end{minipage}

  \caption{\textbf{(a)} A 2D quasi-random sequence from the Sobol
    generator \textbf{(b)} A uniform 2D sequence of pseudo-random
    numbers generated with Mersenne Twister PRNG}
\label{fig:discrepancyplot}
\end{figure}

\emph{TODO applications of QRNGs in numerical integration and Monte Carlo methods}

We will only present a simple inductive formulation of the algorithm,
and it should be noted that more efficient implementations exists
\cite{bratley1988algorithm, hwy2011emerald}. A more detailed
discussion can be found in \cite{dybdalcarlsen2013thesis}.

The algorithm is seeded by a so called \emph{direction vector}. To
generate a Sobol sequence of $32$-bit numbers a direction vector of
length $32$ is required.  Lists of direction vectors are available
online \cite{homepage:sobol:directionvectors}. Let $v$ be a direction
vector of length $32$, the corresponding Sobol sequence is defined by:
\begin{equation}
x_i = v_1i_1 \oplus v_2i_2 \oplus \ldots \oplus v_{32}i_{32}\label{eq:sobol_inductive}
\end{equation}
where $i_j$ is the $j$'th bit in the binary representation of index
$i$. This sequence is then normalised to the $(0,1]$ interval.

It is not hard to translate Definition (\ref{eq:sobol_inductive}) to a
Haskell function working on lists:
\begin{verbatim}
sobol :: [Int32] -> Int32 -> Float
sobol v = normalise . foldl xor 0 . zipWith (*) v . toBitVector
\end{verbatim}
where \verb|toBitVector :: Int32 -> [Int32]| converts the index into
its binary representation and \verb|normalise :: Int32 -> Float|
normalises to $(0,1]$ by division with $2^{32}$.

To generate a complete sequence we can map over all indices:
\begin{verbatim}
sobol1D :: Int32 -> [Int32] -> [Float]
sobol1D m v = map (sobol v) [1..m]
\end{verbatim}
Higher-dimensional sequences can be generated by an additional map:
\begin{verbatim}
sobolND :: Int32 -> [[Int32]] -> [[Float]]
sobolND m vs = map (sobol1D m) vs
\end{verbatim}

We now turn to implement the same algorithm in Accelerate. It should
be noted that the problems we will present are not unique for
Accelerate. We would have more or less the same troubles doing it in
Nikola, Repa, ...  TODO. We use Accelerate for the presentation
because its types are simplified by not including array
representations. 

The type of Accelerate arrays are indexed by their dimensionality,
thus \verb|Array DIM2 Int32| is a two dimensional array with
\verb|Int32| values. Accelerate arrays can not be irregular, and the
two dimensional array will thus be rectangular.

In the first step, there are only minor changes in the type:
\begin{verbatim}
sobolA :: Array DIM1 Int32 -> Exp Int32 -> Acc (Array DIM0 Float)
sobolA v = normalise . foldl1 xor . zipWith (*) v . toBitVector
\end{verbatim}

The next step becomes more complicated though, as Accelerate forbids
nested maps. We will not be able to write \verb|map (sobolA)|.
Instead we will have to push this map inside the definition of
\verb|sobolA|, what a manual vectorisation of \verb|sobolA|.
\begin{verbatim}
sobol1DA :: Exp Int32 -> Array DIM1 Int32 -> Acc (Array DIM1 Float)
sobol1DA m v = normalise $ foldl1 xor $ zipWith (*) rep bitVectors
  where
    Z :. i = arrayShape v
    rep = replicate (constant $ Z :. m :. All) (use v)
    bitVectors = generate TODOTODOTODOTODO
\end{verbatim}
What we have done is to vectorise every operation used in
\verb|sobolA|, such that they operate on values of an additional
dimension.

If we wish to generate $n$-dimensional Sobol sequences, we will again
face the same barrier and we would have to do a vectorisation in the
direction vector argument of \verb|sobol1DA|. Where we had two
dimensional arrays we will have to use three dimensional arrays. We
will not present a \verb|sobolNDA|, but just mention that in all
parts of the sobol1DAcc above we would have to take the additional
dimension into account.
% \begin{verbatim}
%   sobolAccND :: Exp Int32 -> Array DIM1 Int32 -> Acc (Array DIM2 Float)
% \end{verbatim}

This example is a small scale illustration of a problem in languages
that disallow nested array operations. The fact that we could not
apply \verb|sobolA| in the context that we wanted, but had to change
its inner workings showcases a lack of composability and abstraction
in such languages. In a larger setting, such as the one we will
present in the next section, the function that we want to map could be
part of a library which we could not modify ourselves, and thus we
would have to reimplement the complete functionality in our own
program instead.

\emph{TODO what was the problem with using generate instead of map in the
above examples?}

\emph{TODO how does the CUDA code generated by Accelerate look after
  fusion in the above case? Nikola code is fused into a single kernel}

\emph{TODO present benchmark of calculating $\pi$ using Sobol-sequences}

% Disposition for Case I: Sobol generation
% \begin{itemize}
% \item Introduce the simple inductive algorithm for computing
%   Sobol-sequences. Note that more efficient algorithms exists, but
%   we use this simple algorithm for illustrative purposes. See a more
%   detailed explanation in our master's thesis.
% \item Present how the algorithm would be written in a language
%   allowing nesting and compare with the Accelerate version
%   Write that the Nikola version is similar.
% \item We should compare the generated CUDA code by Nikola and
%   Accelerate and make a remark on that
% \item Introduce the concept of "manual vectorisation".
% \end{itemize}

\section{Case II: Binomial option pricing}
\emph{Option} is the name used collectively for a range of different
financial contracts which are time-limited opportunities to buy or
sell some underlying asset such as a stock to a specific \emph{strike
  price} given by the contract.

A particular type of options, American options, are
characterised by having a pre-determined constant strike price $K$,
and may be exercised in a time-interval up until the expiration time
$T$. A \emph{call option} on asset $A$ grants the right to buy $A$,
whereas a \emph{put option} grants the right to sell.

To estimate the price of an \emph{American style option} we have to
simulate the price development of the underlying, as no closed-form
analytic solution is currently known. For the simpler case of
\emph{European style option} pricing a closed-form solution is found
in the Black-Scholes formula \cite{black1973pricing}.

A relatively simple discrete time model for computing the price of an
American style option is the \emph{standard binomial model}
\cite{cox1979option}.  The basic assumption is that the price of the
underlying follows a binomial process over equally spaced time
steps. This makes it possible to write out the possible future states
of the underlying. Moving a single time step forward, the binomial
process produces two possible future states of the underlying. The
value of the underlying can go either up or down with probabilities
$q$ and $1 - q$ respectively. We denote the rate of up and down
movement as $u$ and $d$ respectively. The change over one period
$\Delta t$ is thus given as:

\begin{equation}
S(t+\Delta t) = \left\{
  \begin{array}{ll}
    S(t)u & \quad \textrm{with probability $q$} \\
    S(t)d & \quad \textrm{with probability $1-q$}
  \end{array} \right.
\end{equation}


\begin{itemize}
\item Present binomial option pricing algorithm
\item Show how that would lead to host synchronisation in each iteration
\item See if we can find some paper about the overhead incurred by
  extraneous kernel launches
\item Say that an alternative parallelisation scheme prices several
  options simultaneously: portfolio pricing.
\item Show that we have a hard time expressing such a portfolio
  pricer, because of irregularity when performing the manual
  vectorisation
\item Present a benchmark showing the benefit of having portfolio
  pricer rather than pricing each option in their own kernels
  sequentially.
\end{itemize}

\section{Related work}
\begin{itemize}
\item Mention NESL, Data-parallel Haskell and GPU-NESL
\item Mention some of the languages we haven't tested. See if can put
  them in categories where the above problems are present and aren't,
  even though we haven't tested them as much.
\end{itemize}

\section{Future work}
\begin{itemize}
\item Extend the survey to cover additional languages and problems
\item Create a larger performance benchmark of the different languages
\item Look closer at GPU-NESL and whether it performs
\end{itemize}

\section{Conclusion}
Summarise.

\section{Acknowledgments}

\bibliographystyle{plainnat} 
\bibliography{../bibliography/bibliography}

\end{document}
