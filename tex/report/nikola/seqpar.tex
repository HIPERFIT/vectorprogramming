\chapter{Par/Seq evaluation}


Various ways exist to treat the problem of nested parallelism, with the
extremes being disallowing nested parallelism at all and the vectorisation
transformation of NESL \cite{nesl}.

Nikola itself is able to handle some degree of nested parallelism by exploiting
the up to three-dimensional grid layout of CUDA threads.

\todo{Cite CUDA documentation, and make sure to construct a test case that
actually breaks nikola because of this.}

We propose a different compromise, based on viewing nikola programs a sequence
of parallel actions, that are themselves composed only of sequential parts.  If we instead of
\texttt{for}-loops consider having \texttt{fold}'s, \texttt{reduce}'s, \texttt{map}'s
and \texttt{scan}'s and \texttt{unfold}'s as looping primitives, we realize that
we are able to assign either a 'sequence of parallel' or a 'parallel sequential'
execution to each of them.

\todo{maybe not such a big list of primitives, but particularly 'reduce' and
'scan' seem good for making a point of having separate parallel and sequential
versions, so let's keep them along for now.}

The expression \texttt{map f xs} may for instance be executed in two ways,
assuming that \texttt{xs} is already evaluated:

\begin{itemize}

\item The \texttt{map} happens all elements at once, while \texttt{f} is
evaluated sequentially

\item The \texttt{map} happens one element at a time, while \texttt{f} may itself
be evaluated as a sequence of parallel operations.

\end{itemize}

Which of the above is the more beneficial depends entirely on the precise
operations denoted by \texttt{f} in conjunction with the capabilities of the
hardware that is to execute them.

We propose to adress this by introducing a new primitive construct to Nikola
that we have so far named \texttt{sequential}, and by assigning the (informal)
operational semantics that parallelism in nested cases 'flows inward' until the
first occurence of \texttt{sequential}.

As an example, consider the case of a nested occurence of \texttt{reduce}
inside a version of \texttt{map} that operates on arrays with shapes.

\todo{Should probably end up referencing the earlier section of nikola
operators. Also, this presupposes the existence of \texttt{:+:}. Also, not at
all done yet! Any take on a more instructive example is greatly appreciated.
preferably one using a more likely-to-be-imlemented construct than
\texttt{reduce}}

\begin{verbatim}
map :: sh' -> (Array r'' sh a -> Array r' sh' b)
          -> Array r'' (sh :. Ix) a -> Array D (sh :+: sh') b

rowsum1 :: Array r (Z :. Ix :. Ix) Double -> Array D (Z :. Ix) Double
rowsum1 mat =
  let Z :. rows :. cols = extent mat
  in map Z (\row -> scalar $ reduce (+) 0.0 row) mat

rowsum2 mat =
  let Z :. rows :. cols = extent mat
  in map Z (\row -> sequential $ scalar $ reduce (+) 0.0 row) mat

rowsum3 mat = sequential $
  let Z :. rows :. cols = extent mat
  in map Z (\row -> scalar $ reduce (+) 0.0 row) mat
\end{verbatim}

In \texttt{rowsum1} the intention is that the \texttt{map} is executed
sequentially, while the innermost parallelizable construct, \texttt{reduce},
gets executed in parallel \todo{cite parallel reduce/scan source. Look through
TIPL-2012 papers for inspiration}, exploiting the assumed associativity of the
given operation to reduce with.  In \texttt{rowsum2} the \texttt{map} happens
in parallel, running a sequential reduction for each row in the matrix. In
\texttt{rowsum3} the entire computation is dictated to happen sequentially.
The relative work/depth complexities of these three variations are given in
this table:

\begin{verbatim}
rowsum1  w(cols*rows)  d(rows*lg(cols)) <-- be sure to look up the actual depth complexity of reduce in Nesl!
rowsum2  w(cols*rows)  d(cols)
rowsum3  w(cols*rows)  d(cols*rows)
\end{verbatim}
\todo{turn the above into a proper table}

On a parallel architecture, it is quite probable that different running times
will be observed for each of the three variants, with the difference between
\texttt{rowsum1} and \texttt{rowsum2} depending on the dimensions of the matrix
\texttt{mat}.

\section{Implementation proposals}

\todo{analyse the differences between a type-based and an untyped
implementation. Specifically, address Ken's concerns of 'sequential'
spillover in library functions.}

\section{Implications for Loop Fusion}
