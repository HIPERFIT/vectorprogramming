\chapter{Evaluation of expressiveness}
%\todo{Introduce the contents of this chapter}
%\todo{Can we find a more precise heading?} - Nope. Not now at least.

In this chapter we discuss and compare the expressiveness of each of the
languages we are considering.  The property of programming language
expressiveness is traditionally associated only with how beautiful and simple
encodings a set of algorithms lend themselves to. But since we are dealing with
a domain of algorithms where high computer performance is prerequisite for
acceptability, the need for control over the management of resources invariably
enters the picture -- at least with current programming technology. In this
survey therefore, resource management features of a programming language may
contribute to the lanugages expressivity.

We evaluate expressiveness of a lanugage by discussing its general features,
and by discussing noteworthy issues we have encountered while implementing our
benchmarks in the evaluated languages.

Although we already know that our eventual choice of language to extend stands
only between Nikola and Accelerate, here we also include CUDA, Repa,
Data.Vector and R to give a better perspective.

\section{CUDA/C}
CUDA/C is close to a one-to-one mapping of hardware capabilities to language
primitives, thus it is possible, though not necessarily easy, to implement anything
that the hardware is capable of running.

Every single resource must be explicitly managed -- only in CUDA version 5.0 is
there even a function call stack, supporting only a maximum of 20 levels of
nesting.  While this manual resource management enables the coding of very fast
algorithms, programs are also more tightly coupled with the parameters of the
hardware, and to assumptions about input size. So even though a CUDA programmer
has access to highly optimised linear algebra libraries such as CUBLAS
\cite{CUBLAS2013}, optimisations such as loop fusion must be coded by hand,
which rules out the use of such libraries. This optimisation often has profound
effects on performance \cite{mainlandhaskell}.

Implementing Algorithm \ref{alg:cuda-binom}, and adapting it to use just a
single option proved to be a substantial and error prone effort for us. This we
attribute to both that programming in CUDA is relatively foreign to us and the
lack of conventional debugging tools, such as a single-step debugger or a
machine simulator. The concrete reason for our trouble was almost always index
miscalculations in for-loops rather than misunderstandings of the general
programming model.

\section{R}

The R programming language is designed as a tool for exploring data sets using
statistics and plotting, and for coding prototypes of data analysis programs.
It is safe to say that the focus of R lies mainly on expressivity in the domain
of statistics and data analysis.

R is a very dynamic language, complete with anonymous functions, general
recursion and an immense library of high level operations. Standard R programs
execute on the CPU in their entirety. There is however an effort to utilise
GPUs in R to some extent through the \texttt{gputools}
package\cite{crangputools}.

R has become popular in scientific fields that are not primarily centered on
computer programming, such as biology and statistics
%\todo{cite}.
That in itself is good testament to the practical usefulnes of the language.

As a result of the extensive library support for this domain in R, the case
benchmarks we use in this language were able to offload much work to external
linear algebra routines.

\section{Data.Vector}

The \texttt{Data.Vector} library augment Haskell with a high performance
implementation of one-dimensional vectors. Vectors come in various flavours
that reside in different module namespaces. There are mutable vectors, which
are only usable from inside the IO or ST monads and must be allocated
explicitly, and there are immutable vectors, which are usable in pure
functional code. There is also a version of vectors working on unboxed values.

This vector library interfaces with the GHC compiler to provide the array
fusion optimisation. Taken together with the use of unboxed vectors one is able
to remove a lot of overhead associated with array and thunk construction, even
from ideomatic, functional Haskell code. 

\section{Repa}

Repa is a library that attempts to bring high performance data parallelism to
Haskell by means of threaded multicore CPU parallelism. As a result, Repa
features does not impose more limits on the expressivity of Haskell in general.

The main contribution of Repa towards this end is that arrays in Repa are
parametrised by their internal representation and shape in the type:
\texttt{Array r sh a}.  This enables using different algorithms for processing
different array representations, and restricting certain operations for certain
representations. For example we have delayed arrays, which are simply
represented by an index domain and a function that maps indices to values.
These arrays support indexing, but each index operation will pay the cost of
computing the function.

Repa provides two main operations for manifesting arrays to memory:
\texttt{computeS} for sequential and \texttt{computeP} for parallel array
manifestation. While this division of execution modes is simple, it opens the
potential for expressing nested parallelism. In Repa, nested calls to
\texttt{computeP} is considered an error. A warning is issued on the terminal,
and the documentation says to expect reduced performance.

A consequence of this is that a hypothetical library using Repa that require
the use of array manifestation internally must provide separate parallel and
sequential versions, as a client program cannot use only the parallel version
if it intends to parallise on a another level of the program.

\section{Accelerate}
\label{sec:language-discussion-accelerate}
Accelerate provides a variety of array operations: Both scans,
segmented scans, folds, permutations, maps and zips, implemented as
parallel algorithmic skeletons. Arrays may be multidimensional,
denoted by a type variable in the same style as Repa.

Reductions (folds) are defined on arrays of arbitrary rank by
performing the reduction on the innermost dimension, yielding an array
with rank one less, or by folding all dimensions into a single scalar
value.  Maps are all elementwise regardless of the shape of the input
array, and scans are only defined on one-dimensional arrays.

Accelerate is characterised by a clean division between the frontend and the
various backends that exist. The Accelerate language is thus completely backend
agnostic, and backends simply export a function such as \hbox{\texttt{run :: Acc a ->
a}.}

Accelerate employs a meticulus partitioning of functions on arrays and functions
on scalar values. Array functions are all embedded in the \texttt{Acc a} type,
while scalars are embedded in the \texttt{Exp a} type. So, everything that is
capable of reducing an array or producing an array is carefully placed inside
\texttt{Acc}, and the functions usable for elementwise computation must
reside in \texttt{Exp}. Thus, the lack of nested parallelism is directly
encoded in the type system.

While this restriction probably makes it easy to ensure efficient execution of
the individual contructs, it impairs the composability of the language
constructs significantly, as the programmer needs to manually transform
algorithms with a naturally nested definition into something that will fit into
the view of Accelerate.

Consider this small example, derived from a problem we actually encountered
while exploring the Sobol sequence generation case:

Suppose we have a function \texttt{f} performing some simple vector operation \texttt{h}:

\begin{verbatim}
f :: Vector Double -> Double -> Double
f as b = foldl (\ak a -> h b ak a) 0.0 as
\end{verbatim}

Now, if we want to do scalar multiplication with a vector of scalars, in
\texttt{Data.Vector} Haskell, this could reasonably be writen:

\begin{verbatim}
g :: Vector Double -> Vector Double -> Vector Double
g as bs = map (f as) bs
\end{verbatim}

If we set out to code this program in Accelerate we would have a slightly different
\texttt{f}, as \texttt{fold} is intrinsically mapped:

\begin{verbatim}
f :: Acc (Array (sh :. Ix) Double) -> Exp Double -> Acc (Array sh Double)
f as b = foldl (\ak a -> h b ak a) 0.0 as
\end{verbatim}

But since Accelerate does not allow nested arrays, we need to model the
\texttt{Vector (Vector c)} type  as \texttt{Acc (Array DIM2 c)}. Thus we end up
wanting the function \texttt{g} below, defined using \texttt{f}.

\begin{verbatim}
g :: Acc (Vector a) -> Acc (Vector b) -> Acc (Array DIM2 c)
\end{verbatim}

But there seems to be no way to construct a high-dimensional array from array
functions of lower dimension. So we need to rewrite \texttt{f} itself:

\begin{verbatim}
g as bs = generate
  (index2 $ (size as) (size bs))
  (\ix ->
  let (Z :. ia :. ib) = unlift ix
  in bs ! (Z :. ib) * as ! (Z :. ia))
\end{verbatim}

Furthermore, this is only possible because \texttt{f} is itself defined by an
elementwise operation. Had \texttt{f} been defined by a reduction such as
\texttt{fold}, the transformation would have been even more elaborate and
require replication of both vector \texttt{as} and \texttt{bs} and zipping to
properly distribute \texttt{b} into the folding operation.

\todo{It would be really nice to have a proper explanation of this, preferably
with an illustration of the replication. But that is definitely beyond our
current schedule}
\todo{Add Accelerate Sobol-example as appendix, reference it here - hand drawing?!}

This sharp division between scalar and array operations is easily the biggest
hindrance to the expressiveness of Accelerate, and therefore very relevant to
our interest in language research. However, as this is a pervasive part of the
architecture of Accelerate, removing the distinction of \texttt{Acc} and
\texttt{Exp} would result in an entirely different language, and every single
backend would have to be rewritten.

The embedding of Accelerate leaves some things to be desired. It is for
instance not easily possible to pattern match on tuples and shapes, as these
need to be properly lifted and unlifted to be used (see above in function \texttt{g}).

Also, lifting using \texttt{lift :: Lift c e => e -> c (Plain e)} uses the
\texttt{Plain} associated type, the definition of which is not shown in the
auto-generated documentation of instances. Although the documentation
generation system is arguably to blame for this, it does nonetheless make it
more difficult to easily use value lifting confidently.

There is no way to define recursive functions in Accelerate. Trying to do so
will result in compilation not terminating.

\section{Nikola}
\label{sec:language-discussion-nikola}

Nikola does not provide the variety of array operations that Accelerate does: Only
a few mapping operations are provided, and an iteration construct.  Nikola has
a lot of overall structure in common with Repa. As with Repa, an array's
implementations and shape are represented in the array type, and the programmer
has access to mutable arrays in addition to the common pure array operations.

To exploit the ability to differentiate according to array representations,
operations such as \texttt{map} and \texttt{zipWith} are implemented using
typeclasses.  While this architecture allows for specialisation of operations
to different array representations, actually using the operations results in quite
elaborate types. These are often too compilcated for type inference to resolve
and require a human programmer to supply a type signature.

Compared to Accelerate, Nikola is embedded more naturally inside Haskell,
leveraging the \texttt{RebindableSyntax} GHC-extension. Also, in our programming
experience we were less required to interact with the value lifting machinery
than we were in Accelerate. Value lifting in Nikola is subject to the same
documentation inadequacies as we encountered with Accelerate.

While Nikola does not present any means for expressing nested parallelism, it
does not explicitly ban it either in the style of Accelerate's
\texttt{Acc}/\texttt{Exp} division. Thus, extending the expressive power of
Nikola in this aspect appears at first to be a less elaborate endeavour than in
Accelerate.

There is no way to define recursive functions in Nikola. Trying to do so
will result in compilation not terminating.

Nikola functions are compiled to CUDA code, linked dynamically into
the running Haskell program and then wrapped as a Haskell function of
the corresponding type.  Exactly which types are possible is derived
from a menu of instances of the typeclass \texttt{Compilable a b},
denoting that a value of type \texttt{a} may be compiled into one of
type \texttt{b}, with \texttt{b} determined by a type signature in the
client program. This typeclass is not yet implemented for a lot of the
possible choices of array representations and combinations. During the
implementation of our case studies, we observed that trying to compile
a function with an instance missing often resulted in very lengthy and
elaborate type errors that were hard for us to decode. Luckily our
ability to decode compilation-related type errors improved over time,
but they are nevertheless still unpleasant.

Nikola also integrates with the array representation facility of Repa, by
implementing a representation for arrays that reside in the CUDA device's
memory. By using this array representation it is possible to have both Nikola
programs and other foreign functions manipulate the same physical memory withot
requiring any memory transfers of data to take place between host and device.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master"
%%% End:
