\section{Longstaff \& Schwartz pricing}
Another approach to American option pricing is manifest in the Least
Squares Monte Carlo algorithm, often shortened to LSM. Here Monte
Carlo simulation is used to generate a corpus of possible price
developments of the underlying asset, and the option price is then
calculated by successively applying linear regression at each time
step from expiration to initiation time.

% In a historical context, Monte Carlo methods is relatively new
% approach to option pricing. Lattice methods where introduced in 1973,
% but only since 

\begin{algorithm}
  \begin{algorithmic}
    \Function{LSM}{$T$, $M$, $N$, $S_0$, $K$, $r$}
    \State $\Delta t \gets \frac{T}{M}$
    \State $S \gets$ \Call{GeneratePricePaths}{...} \Comment $S$ is a $N \times (M+1)$ matrix of $N$ paths
    \State $C_0 \gets $ \textbf{parmap} $(\lambda s.\ max(K-s, 0)$ $S_N$ \Comment Initialize cash flow matrix
    \For{$t \gets M-1$ \textbf{to} $1$}
    \State{filter away paths not in the money etc. etc.}
    \State $f_t \gets$ \Call{LeastSquares}{...} \Comment Find regression function $f_t$
    \State estimated time values $\gets$ \textbf{parmap} $f_i$ $S_t$
    \EndFor
    \EndFunction
  \end{algorithmic}
  
  \caption{Least Squares Monte Carlo algorithm}
  \label{alg:lsm-algorithm}
\end{algorithm}

Algorithm \ref{alg:lsm-algorithm} presents the LSM algorithm. We have
left out how to generate price paths and perform least squares
regression, these will be treated in more detail in the subsequent
sections.

\subsection{Path generation}
A standard approach to constructing the corpus of price paths is by
sampling paths of geometric Brownian motion.

This can be done by iterative application of the following equation:
$$S_{t+\Delta t}=S_te^{\Delta t(r-\sigma^2/2) + \sigma v\sqrt{\Delta t}}$$ where 
$v$ is a real number drawn from $\mathcal{N}(0,1)$, $\Delta t$ is the
size of the time period, and $\sigma$ is the volatility. Volatility is
the standard deviation of the option value over time and corresponds
to $u$ and $d$ in the binomial method. The right-hand side consists of
two parts where the first exponent corresponds to the deterministic
evolution of the price by the riskless interest rate, and the second
part corresponds to the variation introduced from the Brownian
motion. The equation is derived in \todo{add citation -- Paul
  Glasserman, section 3.2}

\begin{algorithm}
  \begin{algorithmic}
    \Function{GeneratePricePaths}{$N$, $M$, $s_0$, $\sigma$, $\Delta t$, $r$}
    \State $S_0 \gets$ Initialize vector with $N$ repetitions of $s_0$
    \For{$i \gets 0$ \textbf{to} $M-1$}
      \State $V \gets$ Generate $N$ normally distributed random numbers
      \State $D \gets$ \textbf{parmap} $(\lambda v.\ e^{\Delta t(r-\sigma^2/2) + \sigma v\sqrt{\Delta t}})$ $V$
      \State $S_{i+1} \gets$ \textbf{parallelZipWith} $(+)$ $S_{i}$ $D$
    \EndFor
    \State \Return S
    \EndFunction
  \end{algorithmic}
  \caption{Brownian motion path generation}
  \label{alg:lsm-pathgeneration}
\end{algorithm}

\todo{perhaps introduce Brownian Bridge alternative}

As long as we can generate independent random numbers in parallel this
generation can also be parallelized, as each path can be generated
independently.

\subsection{Random number generation}
A necessity for all applications of Monte Carlo methods is a source of
good random numbers. We cannot computationally generate true random
numbers, as they will always follow our chosen algorithm, but through
the use of \emph{pseudo-random number generators} (PRNGs) we can
generate sequences of numbers with approximately the same properties
as a true random sequence. Such algorithms are widely used in both
Monte Carlo simulation and cryptography, but the choice of algorithm
depends on the application area as different PRNGs displays different
statistical properties.

An alternative scheme is that of \emph{quasi-random number generators}
(QRNGs). Algorithms in this category aims for sequences with low
discrepancy, which is a measure of spacing in the sequence. If we are
to generate a sequence over some interval $[a,b]$, discrepancy would
be high if there were subintervals $[c,d]$ with proportionally few
samples compared to other intervals. Low discrepancy thus guarantees
that proportionally equal amounts of samples are made in all
subintervals. We thus avoid seing gaps with few or no samples and
regions with high sample density. To illustrate, Figure
\ref{fig:discrepancyplot} shows a graph of a low-discrepancy sequence
compared with sequence of uniformly generated numbers from a PRNG.

\begin{figure}
	\centering
	\subbottom[]{\hspace{0.5cm}\missingfigure[figwidth=0.3\textwidth]{Sobol}\hspace{0.5cm}}
	\subbottom[]{\hspace{0.5cm}\missingfigure[figwidth=0.3\textwidth]{PRNG sequence}\hspace{0.5cm}}

    \caption{\textbf{(a)} A 2D sequence of low-discrepancy numbers generated using
      the Sobol generator \textbf{(b)} A uniform 2D sequence of pseudorandom numbers generated with ??}
\label{fig:discrepancyplot}
\end{figure}

QRNGs does not assure any other properties of random sequences such as
\todo{...}, and the aim is not to get close to truly random numbers.

\todo{usage for numerical integration, simulation and optimization}
\todo{usage QRNG in financial Monte Carlo simulation, cite Chaudhary, eric
  couffignals Master Thesis and the d-fine Master Thesis, see also
  \url{http://en.wikipedia.org/wiki/Quasi-Monte_Carlo_methods_in_finance}}

We will look closer at one particular low-discrepancy generator,
namely the Sobol sequence generator, by the Russian mathematician
I.M. Sobol \cite{sobol1967}. 

\todo{why we choose Sobol over other QRNGs}

\subsection{Sobol generator}
We will not discuss why the numbers generated by the Sobol generator
maintain low discrepancy, but only focus on the algorithmic aspect of
computing the numbers.

Given a sequence of so called \emph{direction numbers}, $v_1, v_2,
\ldots$, the computation of Sobol-sequences is pretty straight forward:
$$x_i = b_1v_1 \oplus b_2v_2 \oplus \ldots$$
where $\ldots b_3b_2b_1$ is the binary representation of $i$. This
simple algorithm is presented in Figure \ref{alg:sobol-inductive},
where \textsc{ToBitVector} converts an integer $i$ to its binary
representation as an array of integers. As each iteration is
independent the algorithm is easy to parallelise.

\begin{algorithm}
  \begin{algorithmic}
    \Function{SobolInductive}{$v$,$n$}
    \For{$i \gets 0$ to $n-1$}
    \State $A[i] \gets$ \textbf{fold} $\oplus$ 0 (\textbf{zipWith} ($\times$) (\textsc{ToBitVector} i) $v$)
    \EndFor
    \EndFunction
  \end{algorithmic}
  \caption{Inductive Sobol generator.}
  \label{alg:sobol-inductive}
\end{algorithm}

If one seeks for a sequential algorithm, we can improve by \todo{ref}
using the Gray code binary representation instead of the ordinary,
whereby a recursive definition of Sobol sequences can be made, as it
has been shown that
$$x_{i+1} = x_n \oplus v_c$$
where $c$ refers to \todo{...}

You will not obtain the same sequence using this approach though, but
it has been shown \todo{cite} that Gray code based sequences also
displays the property of low discrepancy. As conversion to Gray code
representation is done through single shift and an exclusive-or
operation, this makes for a cheap sequential algorithm.

The two approaches can be combined to achieve an alternative parallel
implementation. The sequence that needs to be generated is divided in
chunks onto each processing element, the inductive algorithm is used
to initialize each of the chunks and the recursive algorithm is then
used to fill out the space. The algorithm is shown in Figure \ref{alg:sobol-parallel-1}

\begin{algorithm}
  \todo{add gray code, define $v_c$}
  \begin{algorithmic}
    \Function{SobolInductive}{$v$,$n$}
    \State $m \gets$ number of processor core
    \State $r \gets \lfloor n/m \rfloor$
    \ParFor{$p \gets 0$ to $m-1$}
    \State $A[p\cdot r] \gets$ \textbf{fold} $\oplus$ 0 (\textbf{zipWith} ($\cdot$) (\textsc{ToBitVector} i) $v$)
    \For{$i <- p\cdot r + 1$ to $p\cdot r + min(r, n-r\cdot p)$}
    \State $A[i] \gets A[i - 1] \oplus v_c$ 
    \EndFor
    \EndParFor
    \EndFunction
  \end{algorithmic}
  \caption{Parallel Sobol sequence generator.}
  \label{alg:sobol-parallel-1}
\end{algorithm}

The problem with this algorithm is that its memory behavior is not
suitable for GPUs. Programs that does not coalesce memory accesses,
such that all cores act on the same block simultaneously, incurs a big
performance penalty. 

Luckily, an alternative implementation has been found by Thomas
Bradley et al. and is explained in \todo{cite parallelisation
  techniques for random number generators}. They have found that the
...


\subsection{Least Squares solvers}
\todo{LU, QR and Cholesky decomposition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../master"
%%% End:
