\chapter{Iterative array construction}
\label{chap:unfold}
% Disposition
% * Chapter overview
% * Define our need
% * Tell that there are two possible solutions
% * Present solution one: NDP approach
%   - Show where it breaks down, what are we missing
% * Present solution two: Accelerate approach
%   - Tell how it could be used in Accelerate to solve our problem
%   - That it would be possible to implement directly into the Accelerate architecture
% * Discuss the two approaches
%   - With one world view one is better, with another the other is better.
%   - Depends on the point of view

% (* Tell that we implemented a version in Repa
%   - It did what was necessary for us.
%   - We believe it is an addition that might be worthwhile,
%     as we think it is general enough to suit other problems
%   - Show graph of execution speed versus the best performing
%     solution we have without the new construct)

In this chapter we present our takes on adding support for expressing
iteratively defined arrays. Unfortunately we have not been able to get any of these
extensions completely ready for use and possible inclusion into Nikola.  We
will however give an analysis of what is lacking for a more complete
implementation.

Our need to iteratively define arrays is motivated directly from our case work
on the variants of the recursive Sobol sequence generator, depicted in
Algorithms \ref{alg:sobol-recursive}, \ref{alg:sobol-parallel-2} and
\ref{alg:lsm-pathgeneration}. While the specific needs for a specific example
does not always warrant language extensions, the case of incremental
construction of output data is arguably a common one.

But in isolation, the ability to construct a single array iteratively is
insufficient. It must also be possible to produce multiple iteratively
constructed arrays in parallel. This due to both the desire for performance and
the desire for expressivity. To meet this end we will analyse two possible ways
to go about this: a nested parallel version and a flat parallel version.

\section{Nested data parallel version}

Since it is not possible to use recursion in Nikola, we must implement a
primitive iterative operator to incrementally construct arrays.

In Haskell, \texttt{unfoldr} has the type \texttt{(b -> Maybe (a,b)) -> b ->
[a]}.  \texttt{unfoldr f a} recursively populates the element of a list with
\texttt{f}, as long as it evaluates to \texttt{Just (a,b)}. It is a well known
way to construct a list, so we pursue that as an extension.

In Nikola the length of an array is the only intrinsic property shared across
all array representations. Therefore we must know the length of the result array a
priori to evaluating a Nikola \texttt{unfoldr}.

For a prototype that only uses the last generated element as state, we settle
for the type signature:
\begin{verbatim}
unfold ::
  Shape sh =>
  IsElem a =>
  Typeable a =>
    sh ->
    (sh -> a -> a) ->
    a ->
    Array PSH sh a
\end{verbatim}

In order to be useful however, \texttt{unfoldr} by itself is not enough.
Iteratively producing entire rows or columns of a matrix as required in
algorithm \ref{alg:lsm-algorithm} would require the function to be unfolded to produce
arrays, which is impermissable.

To remedy this, we set out to explore nested maps. Again, lacking nested arrays
we must use array shapes to express the structure of our computation.  Like in
the case of \texttt{unfoldr} we thus must express the resulting shape beforehand.

The type we eventually arrived at for our maps capable of nesting is depicted
here:
\begin{verbatim}
mapNest ::
  Shape sh =>
  Shape sh' =>
  IsElem a =>
  IsElem (Exp t Ix) =>
  IsElem b =>
  Source r a =>
      sh' -> (Exp t Ix -> Array D sh a -> Array PSH sh' b)
          -> Array r  (sh :. Exp t Ix) a
          -> Array PSH (sh' :. Exp t Ix) b
\end{verbatim}

While we require the shape \texttt{sh'} to be specified, there is no guarantee
in the application \texttt{mapNest f xs} that \texttt{f x} will not produce an
array exactly the size denoted by \texttt{sh'}. If the resulting array is
smaller than denoted by \texttt{sh'}, an eventual manifestation of the map to
memory will leave some cells uninitialized. Similarly, if the resulting array
is not bounded by \texttt{sh'}, manifesting it will result in writing out of
bounds.

For now we have chosen to accept the existence of these fallacies, partly
because we are concerned with implementing a prototype, and partly because we
don't see any acceptable resoultion of the problem beyond placing the actual
array extent rather than just the shape into the type of the array.

Another issue with this definition is the \texttt{Source r a} restriction on
\texttt{xs}. If the mapped function does not need to index into the array, it
is an unnecessary restriction to impose.  Similarly, if the mapped function
performs only elementwise operations, preserving the porperty of being
indexable would be valuable to retain for possible further processing of the
result array.

Risking to further complicate the types, the situation is somewhat remediable
by defining \texttt{mapNest} as a function in a typeclass parametrised over the
involved array representations, as is already the case for typeclass
\texttt{Map} in module \texttt{Data.Array.Nikola.Operators.Mapping}.

The most important issue about this definition however, is that it permits
expressing nested parallelism, which is not directly executable on the flat
parallel hardware. Treating this issue is the subject of Chapter
\ref{chap:directing-parallelism}.

\subsection{The inadequacies of our implementation}

All our efforts on iterative array construction draw on Nikolas implementation
of 'push' arrays. As mentioned earlier, delayed arrays and push arrays are each
others opposites. Delayed arrays are \texttt{Source} arrays, and their
manifestation is at the mercy of the consumer of the array -- Therefore it is
required that the elements of a delayed array are completely independent from
each other.  Push arrays on the other hand are defined as a stream of
index-value pairs, and thus need not obey the same restriction on element
dependence.

Push arrays in Nikola are defined by means of the for-loop constructor
\texttt{ForE} of \texttt{S.Exp}. This loop constructor however, is very
low-level in the sense that it includes as arguments a list of variables to be
looped over and a list of loop bounds corresponding to each of the variables,
and corresponds directly to a for-loop in C.

% The \texttt{Shape} typeclass defines an accessible interface for constructing
% for-loops that visit each index of given shape to loop over.  The operation
% \texttt{seqfor :: Shape sh => sh -> P sh} that we use for constructing our
% loops is defined using the \texttt{shift} action, and thus needs to be paired
% with an enclosing \texttt{reset}.  Due to the way that \texttt{shift} operates,
% what happens is that upon issuing \texttt{seqfor} in the \texttt{P} monad, the
% entire surrounding action up to the nearest enclosing reset is placed inside
% the body of the for-loop defined by \texttt{seqfor}.

However to our regret, using this approach for defining \texttt{unfold} and
\texttt{mapNest} proved unfruitful, as the part of the C code generation that
pertains to for-loops in Nikola does not handle cross-iteration data
dependencies properly. It assumes that variables that are overwritten inside
the body of the loop constitute new, fresh variables. Due to our time
constraints we were not able to address this, and have to leave our operations
be for now. If however we disregard this problem, we are not aware of any more
obstacles for implementing our extensions properly.

\section{Flat data parallel version}

Our first attempt at defining an unfolding operation was inspired by the
collective operators of Accelerate.

\begin{verbatim}
unfold ::
  IsElem a =>
  IsElem (Exp t Ix) =>
  Shape sh =>
  Source r a =>
    Exp t Ix -> (Exp t Ix -> a -> a)
    -> Array r sh a -> Array PSH (sh :. Exp t Ix) a
\end{verbatim}

By avoiding the need for a nested map, it is guaranteed that the resulting
unfolded arrays all have the same size. Furthermore, since the operation is a
mapped version of \texttt{unfold}, the outer structure of the computation is
completely static, and it is possible to assign a parallel execution semantics,
given that the unfolded function is executed sequentially.

As such it fits quite well with the other array operations in Accelerate. If it
were to be implemented in Accelerate, it would enable us to express the
optimised version of Sobol sequences, Algorithm \ref{alg:sobol-parallel-2},
which is currently inexpressible.
