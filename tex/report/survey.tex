\part{Survey}
\chapter{Survey Setup}
In this and the following chapters we present a small-scale survey of
current vector languages. The survey was originally conducted to
orient ourselves in the current landscape of parallel functional
languages and their implementation, but the results might be of
interest for others. We do not know of other comparisons with similar
scope.

We have decided to evaluate and compare the languages Accelerate
\cite{chakravarty2011accelerating}, Repa \cite{keller2010regular},
Nikola \cite{mainland2010nikola} and the \texttt{Data.Vector}
package\footnote{\url{http://hackage.haskell.org/package/vector}}. We
compare these four libraries to the R programming language and NVIDIA's
CUDA platform, which are some of the languages currently used by
financial engineers. The R language is used for expressing financial
algorithms succinctly, while CUDA is used for performance reasons,
which is why we have chosen to include both of these. Because of time
constraints, we have had to limit ourselves more than we had originally
intended. This means we have had to leave out languages such as
Feldspar\cite{axelsson2010feldspar},
Obsidian\cite{svensson2011obsidian}, Data Parallel Haskell \cite{},
Copperhead\cite{Catanzaro2011} and NESL\cite{nesl} from the survey,
although they might have provided further insights.

\todo{Why only Haskell? Are there any languages we have left out
  entirely? What about Theano, ArBB, Qilin, Erlang, SaC, CnC-CUDA? Why
  aren't they here?}

\section{Language introduction}
\todo{Introduce the basics of Nikola, Accelerate, Repa and Data.Vector}

\section{Programming language surveys}
To make a fair comparison, we have to be objective in our evaluation
of the languages. Obtaining objectivity in such a comparison is not
an easy task, as it is hard to quantitatively measure aspects such as
the quality of language documentation (longer is not always better,
and it might be outdated). Also, as mentioned by Lutz Prechelt in his
2001 paper ``Empirical Comparison of Seven Programming Languages'':

\begin{quote}
  ``Any programming language comparison based on actual sample programs
  is valid only to the degree to which the capabilities of the
  respective programmers using these languages are similar.''
\end{quote}

We would thus have to either acquire the same level of experience in all the
languages ourselves or find experts in each of the languages to do the
implementation. We have not had the resources to conduct a survey
following the standards set in the paper by Lutz Prechelt. For each
language under comparison he acquired 10-20 implementations of the
same algorithm from different programmers (mostly graduate
students). We could have set up a similar experiment by presenting a
programming challenge to the ``Haskell-Cafe''-mailing list and/or the
Haskell section on the Reddit website, and surely we could perhaps get
a decent benchmark in terms speed and memory usage for different
implementations by different developers, but we would not get answers
to qualitative questions about the development process. Another aspect
is that these languages are all in their early stages, and most people
we would find on those channels might be amateurs. We do not find
amateur work a good basis for an objective comparison.

\todo{could still be an interesting experiment to make in the future.}
\todo{look through the above argument again}

\section{Comparison}
From the survey we want to uncover three main questions about each
language, and it is from answers to these questions that we will make
a comparison. The three questions revolve around the health of
project, the expressiveness and ease of use of the language, and the
performance of the language. The following three sections will pose
these questions and describe how we have decided to test them.

\todo{Should we also include an evaluation on ``Performance
  \textit{guarantees}''? That is, an evaluation of the theoretical
  aspects and cost models?}

\subsection{Project Health}
How good is the project health? That is,
we want to determine how likely is it that development on the language
will continue and that it will keep getting funding and interest from
developers.

Subquestions include:
\begin{itemize}
\item Quality of documentation. Good documentation gives better acessibility for new users and developers.
\item Code review. Our own assesment of the standard of the code.
\item Portability. More portable progams have a wider potential user audience.
\item Installation process. A cumbersome installation process is deterring for newcomers.
\item Number of dependencies (incl. GHC extensions). ``Bad health is transitive''.
\item Number of users (reverse dependencies). The bigger the audience, the more people with some interest in the project continuing.
\item Number of contributors.
\item How easy is it to start contributing?
\item Latest project activity (e.g. which version of GHC does it compile on?)
\item Licensing
\item Funding
\end{itemize}

\todo{Document the procedures and scripts that were used to collect reverse dependencies and to carry out installs}

\subsection{Ease of use and expressiveness}

How easy is the language to use? And how expressive is it? Our
aim is to find a language that might be suitable for a financial
engineer and we thus want a suitably high-level language. It should
be near the complexity level of R. We still want the language to
expressive enough to cover the domain of financial algorithms.

Subquestions include
\begin{itemize}
\item Which programs can we write, and which can't we write?
  \begin{itemize}
  \item Can we write nested loops?
  \item Does it include all of: scans, folds, zip/map, stencils,
  \end{itemize}
\item How good are the error messages?
\item How high-level is it on the scale from ``R'' to ``CUDA''?
\end{itemize}

\subsection{Performance} How does the languages compare in a
performance benchmark?
\begin{itemize}
\item Benchmark of binomial pricer on expiry = 1,2,4,8,16,32,64,128 years.
\item Benchmark of Longstaff and Schwartz
\item Which optimizations are performed?
\item How many in-code optimiser hints (inlining-statements, forcing
  of delayed arrays etc.) are necessary to get decent performance?
\item How does the performance of a naive implementation (no
  optimiser hints) compare to an optimised version?
\end{itemize}


%\begin{itemize}
%\item How do we carry out the survey?
%\item How are such surveys normally performed?
%\item Which parameters do look for and measure?
%\item How do we measure these things, e.g. how do we evaluate ``project health''?
%\item Why didn't we include some languages, but included others?
%\end{itemize}

\chapter{Evaluation: Project Health}
%``How good is the documentation?'',

\section{Accelerate}
\subsection{Source Quality}

\paragraph{Documentation.} The documentation for Accelerate resides mainly in
two different places: The haddock API-docs on hackage, and more high-level
documentation on the project's github wiki.  Other documentation reside in
somewhat scattered web pages, most notably in the paper
\cite{chakravarty2011accelerating}.  As of this writing, the main introductory
material seems to be the github project wiki and the paper
\cite{chakravarty2011accelerating}. Of these two, the wiki pages are more
oriented towards building applications using Accelerate, while the paper
focuses on detailing the current implementation and only briefly introduces the
Accelerate language in an academic fashion.

Unfortunately, many of the wiki pages are rather incomplete, so we must
conclude that there has yet to be published any comprehensive introductory
material on programming using Accelerate.
The API reference documentation however is extensive.

\paragraph{Code.} The Accelerate codebase is large for a Haskell library, and
a bit overwhelming at first. They have a good division between frontend (the
accelerate package) and backends (e.g. the accelerate-cuda package), which
limits the amount of code you have to get into. You don't really have to
understand the complete frontend to modify in the backend and vice versa. A
good overview document of the different modules, and their tasks would be a
great benefit for getting other hackers involved.\todo{Text above is copied
verbatim from wiki.}

\paragraph{Installation process} We succeeded in getting Accelerate to run on
GHC 7.4.2, but it still doesn't support GHC 7.6.1, possibly because of external
dependencies.  We had some trouble installing the version on hackage, because
of a minor bug in the cuda-bindings. The bug was caused by code generated by
c2hs, so it might be something that depends on the version of CUDA installed.

We had major trouble installing the development-version from their repository,
mainly caused by a lot of dependency issues and conflicts. We still have to
compile the CUDA-backend.  \todo{Text above is copied verbatim from wiki.}

\subsection{Relations with other projects}

\paragraph{Dependencies.}
For the Accelerate frontend we have counted 8 package dependencies and 19 GHC
extensions, while the CUDA backend sports 23 package dependencies and 5 GHC
extensions.
Most of the package dependencies are commonly occuring packages, but some are
less so. \todo{The last paragraph sounds unsubstantiated}

The Accelerate frontend is tied only to the GHC-compiler (through its generous
use of GHC haskell extensions), and thus runs on any platform (architecture or
OS) that GHC supports.
The CUDA backend is only available on machines with a NVidia CUDA graphics
card, and supports all versions of CUDA (but recommend using at hardware with
at least compute capability 1.2).

\paragraph{Reverse Dependencies.} For Accelerate we have
counted a total of two package users on hackage, excluding the different
accelerate backends, as we consider them to be part of the same project.

\paragraph{Contributors.} During the last 12 months, the Accelerate
frontend has seen a total of 6 different contributors. Across the entire
project history, 14 people have contributed.
Accelerate is hosted as a git repository on github.com, so getting a copy of
the latest source code and submitting patches is structurally
straightforward.
Given that the project has seen a variety of small volume contributers, with no
apparent close connection to the Accelerate maintainers, they appear to have an
inclusive attitude towards other people's code.

\paragraph{Licensing.} Accelerate is licensed under BSD-like terms, requiring
mainly that attribution is retained in derivative works.

\subsection{Maintainance}

Accelerate has been actively maintained for the
past 12 months.
The two main contributors, Trevor L. McDonell and Manuel M
T Chakravarty, are both employed by University of New South Wales, Sydney,
Australia, and assumably maintain Accelerate as part of their research there.

\section{Repa}

\subsection{Source Quality}

\paragraph{Documentation.} Repa is extensively documented through both API
documentation on hackage, various papers, tutorials and example programs, eg in
\cite{lippmeier2012guiding} and \cite{keller2010regular}.  Everything is easily
accessible directly from the front page of the repa homepage
\cite{homepage:repa}.

\paragraph{Code.}
\todo{\ldots}

\paragraph{Installation process.} Since Repa is just a regular Haskell package
that doesn't interface with foreign functions, its installation is
straightforward using the \texttt{cabal-install} program.

\subsection{Relations with other projects}

\paragraph{Dependencies.} %
Repa uses 18 GHC extensions and depends on 6 other established packages, none
specific to any platform. Thus, Repa will most probably function on any
machine that is targetable by GHC.

\paragraph{Reverse Dependencies.} Repa is a comparatively popular package. On
hackage there are 9 other packages that each depend on Repa.

\paragraph{Contributors.} Looking at the commit log for the official Repa
source code repository, we may count 4 distinct names. The primary contributor
(in number of patches) appears to be Ben Lippmeier by a great margin.

\todo{It seems redundant to keep saying that the code is publically available
in a VCS. They all are. And we have no reference to cite that contributions are
welcome, so any comment on contribution friendliness would be guesswork. Should
we perhaps report that? Is the question of ease of contribution even
interesting? Isn't it a universally accepted work dynamic of free/oss software
that contribution is encouraged}

\paragraph{Licensing.} Repa is licensed under the terms of the BSD3 license.

\subsection{Maintainance}
Ben Lippmeier is employed as a researcher by "School of Computer Science and
Engineering", and Repa is listed among his projects there.

\section{Nikola}

\subsection{Source Quality}

\paragraph{Documentation.} As Nikola is not published on Hackage, there is no
automatically searchable generated documentation, and one has to manually
generate haddock documentation. Counting source lines reveals a figure of 24\%
comment lines, which is above the average comment ratio according to the
ohloh.net open source project visualisation website\footnote{14-11-2012:
``Across all Haskell projects on Ohloh, 17\% of all source code lines are
comments. For nikola-haskell, this figure is 24\%.''} Further, Nikola is
described in the paper \cite{mainland10nikola}.

\paragraph{Code.} (TODO)

\paragraph{Portability.} Only runs(/builds even?) on machines with an NVidia
CUDA card and with the CUDA drivers and sdk installed.

\paragraph{Installation process} Nikola requires running a classical style
automake "configure"-script to produce the binding to the cuda-backend.

We had a sizable amount of trouble executing the installation. \todo{why exactly?}

\subsection{Relations with other projects}
\paragraph{Dependencies.}
\paragraph{Reverse Dependencies.}
\paragraph{Contributors.}
\paragraph{Licensing.}
\subsection{Maintainance}

\section{Data.Vector}

\subsection{Source Quality}
\paragraph{Documentation.}
\paragraph{Code.}
\paragraph{Portability.}
\paragraph{Installation process}

\subsection{Relations with other projects}
\paragraph{Dependencies.}
\paragraph{Reverse Dependencies.}
\paragraph{Contributors.}
\paragraph{Licensing.}
\subsection{Maintainance}


\begin{table}
  \centering
  \begin{tabular}{l|rrllllr}
    Language    & Project age & Latest release & License & Contributors \\ \hline
    Accelerate  & 3-4 years   & June 2012      & BSD3    & 3 \\
    Nikola      & 1-2 years   & Not released   & BSD3    & 1 \\
    Repa        & 1-2 years   & October 2012   & BSD3    & 4 \\
    Data.Vector & 4 years     & October 2012   & BSD3    & 9 \\
  \end{tabular}
  \caption{Project status}
  \label{tab:project_status}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{l|rrllllr}
    Language    & Dependencies & Reverse dependencies & GHC extensions & GHC version \\ \hline
    Accelerate  & 5 (+18)      & 2                    & 19 (+9)        & 7.6.1 \\
    Nikola      & 21 0         & 0                    & 26             & 7.4.2 \\
    Repa        & 6            & 11                   & 20             & 7.6.1 \\
    Data.Vector & 4            & 150+                 & 15             & 7.6.1 \\
  \end{tabular}
  \caption{Dependency status}
  \label{tab:dependency_status}
\end{table}


\chapter{Evaluation: Expressiveness}
% ``How tight is the correspondence to the Rolfs R-code?''

\chapter{Performance}
We will now look at how the different languages compare in performance
and which optimisations they employ.

\section{Optimisations}

\subsection{Accelerate}
Accelerate employs the following optimisations:

fusion, memory coalesced access



\section{Benchmarks}
Speed up graph. In the graph we use the sequential version written
with Data.Vector as our basis, which the others are compared to.

Accelerate is not included, because of problems in their stream fusion
algorithm leading to running times that evolve
exponentially. Disabling fusion lead to CUDA exceptions.

We see that the overhead incurred by using Nikola makes both
sequential implementations (R and Data.Vector) faster for the smallest
cases, and only when we reach the largest example (128 years expiry
time) Nikola catches up.

Repa is the best performing, even though it does not perform any GPU
computations. We are not sure why Repa gives so varied speed-ups for
different input sizes, but it might be because of our somewhat
arbitrary threshold for which problem sizes should be executed in
parallel and which should be executed sequentially.


\chapter{Conclusion}
What are our recommended choice of languages for different use-cases?

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:

